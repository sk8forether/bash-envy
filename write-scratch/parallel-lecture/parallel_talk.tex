\documentclass[pdf]{beamer}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[english]{babel}
\usepackage[export]{adjustbox}
\usepackage{listings}
\usetheme{Warsaw}
%\mode<presentation>{}

\title{Parallel Computing}
\subtitle{A Quick Primer}
\author{Tim Smith}

%% --- New commands
\newcommand{\pderiv}[3][]{% \pderiv[<order>]{<func>}{<var>} 
  \ensuremath{\frac{\partial^{#1} {#2}}{\partial {#3}^{#1}}}}
  
\newcommand{\red}[1]{\textcolor{red}{#1}}

\graphicspath{{figures/}}

\begin{document}
 %% title frame
 \begin{frame}
  \titlepage
 \end{frame}

 \begin{frame}{Motivation}
  \begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{frustrated}
  \end{figure}
 \end{frame}
 
 %% outline frame
 \setbeamertemplate{section in toc}[ball unnumbered]
 \begin{frame}{Overview}

  \tableofcontents
 \end{frame}
 
%% --------------------------------------------------------------------------------
%% Single Processor concepts
 \section{Single Processor Concepts}
 \begin{frame}{Overview}
  \tableofcontents[currentsection]
 \end{frame}
 \begin{frame}[shrink=10]{Key players on the chip}
   %% \setbeamercovered{transparent}
  On a single processor chip, we have the following components which are crucial to performing calculations:
  \begin{itemize} % [<+->]
	\item \textbf{Clock cycle}: A pulse measurement in Hz which synchronizes computations. Important for determining how many instructions can be handled ``per cycle'' 
	\item \textbf{Floating Point Units}: The hardware designed to perform floating point operations: vector multiply, add, fused multiply-add: $y = ax + b$. Less often optimized for division, which can take 10 to 20 clock cycles over 1 clock cycle for add and multiply.
	\item \textbf{Register}: The closest storage unit to the FPUs. This ``feeds the beast''. Variables in register can be explicitly manipulated by the user.
	\item \textbf{Cache}: The next-in-line memory unit. Comes in multiple stages of increasing distance and storage size. Hidden from user. 
	\item \textbf{Main memory}: The largest memory unit in size but furthest from the FPUs. 

   \end{itemize}
  \end{frame}

 \begin{frame}{Memory Hierarchy}
  \begin{figure}
  \centering
  \includegraphics[width=\textwidth]{memory_hierarchy}
  \end{figure}
 \end{frame}

 \begin{frame}{Data Movement}
  \begin{itemize}
	\item \textbf{Busses}: The wires that move data around. Speed is a fraction of clock cycle speed $\sim$1 GHz, compared to LS 5 Haswell compute nodes at 2.6 GHz.
	\item \textbf{Latency}: Time delay from when a piece of data is requested and when it actually arrives. 
	\item \textbf{Bandwidth}: After initial latency is overcome, a measurement of how quickly data can arrive by \textit{(bus speed)}) x \textit{(bus width)}.
  \end{itemize}
  \begin{block}{Message arrival time}
	\begin{equation}
	  T(n) = \alpha + \beta n
	\end{equation}
	$n \equiv$ the size of data
	$\alpha \equiv$ latency
	$\beta \equiv$ 1/bandwidth
  \end{block}
 \end{frame}

 \begin{frame}{Flow to memory}
  \begin{itemize}
	\item When data is requested, it does not show up as a single byte or word. Instead a \textbf{cache line} is pulled, which is typically 8-16 floating point numbers. 
	\item Thus, access to the rest of the line is effectively free.
	\item Fortran is \textbf{column major}, meaning next data items in a vector or matrix column are pulled as the cache line. C is \textbf{row major}. 
  \end{itemize}
  \begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{cache_line}
  \end{figure}
 \end{frame}

 \begin{frame}[fragile,shrink=10]  
  \frametitle{In the MITgcm}
  \begin{lstlisting}{language=Fortran}
    do k = 1,nr
      do j = 1,sNy
         do i =  1,sNx
           trVolW(i,j,k) =
&                 uVel(i,j,k,bi,bj)*hFacW(i,j,k,bi,bj)
&                *dyG(i,j,bi,bj)*drF(k)*msktrVolW(i,j,bi,bj)
&                *maskInW(i,j,bi,bj)
           trVolS(i,j,k) =
&                 vVel(i,j,k,bi,bj)*hFacS(i,j,k,bi,bj)
&                *dxG(i,j,bi,bj)*drF(k)*msktrVolS(i,j,bi,bj)
&                *maskInS(i,j,bi,bj)

   .
   .	
   .
         enddo
       enddo
     enddo
  \end{lstlisting}
\end{frame}


%% --------------------------------------------------------------------------------
%% Parallel Computing 

 \section{Parallel Computing Concepts}
 \begin{frame}
  \tableofcontents[currentsection]
 \end{frame}

 \begin{frame}{Hardware Terminology}
  \begin{itemize}
	\item \textbf{Core}: A single processing unit. 
	\item \textbf{Socket}: The physical processor chip which holds many cores.
	\item \textbf{Node}: A computing unit containing one or more sockets. Memory is shared across the node. 
	\item \textbf{Cluster}: The server or supercomputer consisting of many nodes.
	\item \textbf{Processor}: an ambiguous term ...
  \end{itemize} 	
  
  Example: 
  \begin{itemize}
	\item Lonestar 5 is a cluster with 1252 compute nodes 
	\item 2 sockets per node
	\item 12 cores per socket (shared memory among 24 cores)
	\item 64 GB memory per node, so $>2$ GB per core in a pure MPI model 
  \end{itemize}
 \end{frame} 

 \begin{frame}{Sandy Bridge Chip}
	\begin{figure}
	\centering
	\includegraphics[width=.6\textwidth]{sandy_bridge}
	\end{figure}
 \end{frame}

 \begin{frame}{Virtual Terminology}
  \begin{itemize}
	\item \textbf{Process}: A running executable
	\item \textbf{Thread}: More abstract, a set of instructions being followed
  \end{itemize} 	
  
  Example: 
  \begin{itemize}
	\item In a typical parallel MITgcm run, one will call $N_{px}\, \mathrm{x}\, N_{py}$ processes each on individual cores with separate memory spaces
	\item One process can open up a number of threads to simultaneously complete a do loop 
  \end{itemize}
 \end{frame} 

%% --------------------------------------------------------------------------------
%% Parallel implementation 
\section{Implementation}
 \begin{frame}{Overview}
  \tableofcontents[currentsection]
 \end{frame}

 \begin{frame}{OpenMP in One Slide}
  \begin{itemize}
	\item Parallelization scheme for \textbf{shared} memory model
	\item Shared memory: over at most one node, a scheduler assigns tasks to threads which are completed simultaneously. All threads work on the same scope of information. 
	\item Parallelization is under the hood
	\item Code is easy to write
  \end{itemize}
 \end{frame}

 \begin{frame}{MPI}
  \begin{itemize}
	\item Parallelization scheme for \textbf{distributed} memory model.
	\item Distributed memory: each process only has a local view of global data set 
	\item Much harder to write code than OpenMP
	\item User must specify how information is passed between processes 
	\item This gives clear picture and full control
  \end{itemize}
 \end{frame}

 \begin{frame}{Workflow}
  A general process for flow for parallel code:
  \begin{enumerate}
	\item Initialize processes, establish rank 
	\item Perform domain decomposition
	\item Do computation
	\item Communicate among processes 
	\item ...
	\item Finalize
  \end{enumerate}
 \end{frame}

 \begin{frame}{Domain Decomposition}
	\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{domain_decomp_2}
	\caption{In the initialization process, the global grid is split into tiles and tiles are assigned to MPI processes. Each process is initialized with a unique identifier called it's \textbf{rank}, this is used in the communication routines.} 
	\end{figure}

 \end{frame}

 \begin{frame}{Domain Decomposition}
	\begin{figure}
	\centering
	\includegraphics[width=.7\textwidth]{domain_decomp_1}
	\caption{For our purposes, each tile is assigned to an MPI process and we use one MPI process per core. Tiles without ocean are neglected, and this is handled by the \textit{exch2} package, reducing the number of cores required.}
	\end{figure}
 \end{frame}

 \begin{frame}{Domain Decomposition}
	\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{domain_decomp_3}
	\end{figure}
 \end{frame}

 \begin{frame}{Point to Point Communication: Halo region}
	\begin{figure}
	\centering
	\includegraphics[width=.7\textwidth]{colHaloExchange}
	\caption{Neighboring tiles exchange overlapping data to update halo region (red and blue). Here $OLx=2$. This must be performed e.g. at each time step for spatial gradients and each time forcing data is updated (every 6 hours in ECCO).}
	\end{figure}
	MITgcm example: exchange scalar across halo region at level r
	exch\_3d\_r8( phi, r, myThid )
 \end{frame}

 \begin{frame}[fragile]
 \frametitle{Collectives}
	\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{collectives}
	\end{figure}
	MITgcm example: allreduce = reduce + broadcast \\
	global\_sum\_tile( local\_data, global\_sum, myThid )
\end{frame}

%% --------------------------------------------------------------------------------
%% Performance
\section{Performance}
 \begin{frame}{Overview}
  \tableofcontents[currentsection]
 \end{frame}

 \begin{frame}{Performance Terminology}
  \begin{itemize}
	\item $p \equiv $ Number of processes
	\item $T_1 \equiv $ Wall clock time on 1 process
	\item $T_p \equiv $ Wall cock time on $p$ processes
	\item Speedup: 
	 \begin{equation}
		S_p = \frac{T_1}{T_p} \leq p
	 \end{equation}
	\item Efficiency: 
	 \begin{equation}
		E_p = \frac{S_p}{p} \leq 1
	 \end{equation}
  \end{itemize}
 \end{frame}

 \begin{frame}{Characterizing Performance: Scalability}
	How well does a code perform on parallel architecture? Consider two types of problems:
  \begin{enumerate}
	\item For a fixed problem size, increase the number of processes to improve the performance. The model here shows \textbf{strong scaling} if the speedup increases linearly with added processes. 
	\item As the number of available processes increases, the problem size increases (e.g. refining grid resolution). Keeping data per process constant, the speed in ops per second on each process remains constant as problem size and number of processes grows if the model shows \textbf{weak scaling}.
  \end{enumerate}
 \end{frame}

 \begin{frame}{ECCO Speedup on Lonestar 5}
	\begin{figure}
	\centering
	\includegraphics[width=.6\textwidth]{ecco_ls5_scaling}
	\caption{Model scales up to 192 cores in an adjoint run, up to 96 cores in forward mode.}
	\end{figure}
 \end{frame}

 \begin{frame}{Strong Scaling}

	\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{wallTime_strong.png}
	\caption{Model scales up to 256 cores, relative to 4 cores. Note how moving from 1 to 4 cores does not give an overall speedup due to communication. These types of issues are problem dependent.}
	\end{figure}
 \end{frame}

 \begin{frame}{ORCA Scaling}
	\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{orca_scaling.jpg}
	\end{figure}
 \end{frame}

% \begin{frame}[shrink=10]{Limiting Factors to Speedup}
%  \begin{itemize}
%	\item Amdahl's law: I/O, parallelization overhead create inherently sequential code. 
%		\begin{equation}
%			T_p = T_1(F_s + \frac{F_p}{p}) + T_c
%		\end{equation}
%		\begin{equation}
%			S_p = \leq 1/F_s
%		\end{equation}
%		\begin{equation}
%			E_p \sim 1/p
%		\end{equation}
%	\item Gustafson's law: sequential fraction usually independent of problem size, define parallel problem and compare to single process doing all work.
%		\begin{equation}
%			T_p = T(F_s + F_p)		
%		\end{equation}
%		\begin{equation}
%			T_1 = T(F_s + pF_p)		
%		\end{equation}
%		\begin{equation}
%			S_p = p(1 - \frac{T_c}{T_1}p)		
%		\end{equation}
%		\begin{equation}
%			T_1 \rightarrow pT_p \implies E_p \sim 1 - \frac{T_c}{T_p}
%		\end{equation}
%		
%  \end{itemize} 
% \end{frame}



 %  \begin{frame}{Weak Scaling}
 %   This is hard to pin down, even when talking to highly experienced people. See \cite{Eijkhout_introToHpc}, section 2.3.5 where a model for performance is formulated which looks at available memory 
 %  \end{frame}

 \begin{frame}{References}
  For sample batch scripts check out my bash-scripts directory: 
  \begin{block}{}
  https://bitbucket.org/timothys/gcm-contrib
  \end{block}
  \begin{thebibliography}{3}
  \bibitem{eijkhouthpc} 
  Eijkhout, Victor. \textit{Introduction to High Performance Scientific Computing}. Lulu. com, 2014.
  \bibitem{eijkhoutparcomp}
  Eijkhout, Victor. \textit{Parallel Computing for Science and Engineering}. 2015.
  \bibitem{mitgcmManual}
  Adcroft, A, et al. \textit{MITgcm User Manual}. MIT Department of EAPS, 2016. 
  \end{thebibliography}
 \end{frame}

\end{document}
